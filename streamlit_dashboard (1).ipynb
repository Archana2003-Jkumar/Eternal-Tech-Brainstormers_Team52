{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5v3QSbBEdNO",
        "outputId": "892611e5-a4af-4775-89ed-b09c53171473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/10.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/10.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m7.2/10.0 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.0/524.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install streamlit pandas numpy folium streamlit-folium plotly pydeck requests pyngrok -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!cp \"/content/drive/MyDrive/assignments_log.csv\" /mnt/data/assignments_log.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bErRiUfc3raZ",
        "outputId": "c19357e9-c0ef-49f7-abf3-7fa3440563ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/assignments_log.csv': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eULwqwC4XDa",
        "outputId": "200ae683-13f7-4449-9399-c51518f3d61e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/assignments_log.csv\" /mnt/data/assignments_log.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD7AiVDn4fZW",
        "outputId": "7ed78ffe-9015-47ec-91b1-80a88cc6b838"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/mnt/data/assignments_log.csv': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "j7VCZdLnE7gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07dZXBvY9OHs",
        "outputId": "6158e3e1-0b15-4da5-8c38-8f30de654ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "# streamlit_app.py\n",
        "%%writefile streamlit_app.py\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "import streamlit as st\n",
        "import pydeck as pdk\n",
        "import plotly.express as px\n",
        "import folium\n",
        "from streamlit_folium import st_folium\n",
        "\n",
        "# ----------------- TomTom & OpenWeather helper code (adapted from your snippets) -----------------\n",
        "\n",
        "# TOMTOM config (reads from env)\n",
        "TOMTOM_API_KEY = os.environ.get(\"TOMTOM_API_KEY\", \"8tnvwDhRN7nFEDHgC78NtgP1bmQye9vp\")\n",
        "if not TOMTOM_API_KEY or TOMTOM_API_KEY.startswith(\"<\"):\n",
        "    # we won't hard-fail at import time; the fetch function will raise if key missing for live mode\n",
        "    TOMTOM_API_KEY = TOMTOM_API_KEY  # keep the provided default\n",
        "\n",
        "TOMTOM_FLOW_URL = \"https://api.tomtom.com/traffic/services/4/flowSegmentData/absolute/10/json\"\n",
        "\n",
        "\n",
        "def get_tomtom_speed_kmh(lat, lon, retries=2, pause_sec=0.5):\n",
        "    params = {\"point\": f\"{lat},{lon}\", \"key\": TOMTOM_API_KEY}\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            resp = requests.get(TOMTOM_FLOW_URL, params=params, timeout=15)\n",
        "            if resp.status_code in (429, 500, 502, 503, 504):\n",
        "                if attempt < retries:\n",
        "                    attempt += 1\n",
        "                    time.sleep(pause_sec * (attempt + 1))\n",
        "                    continue\n",
        "                resp.raise_for_status()\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json()\n",
        "            fsd = (data or {}).get(\"flowSegmentData\", {}) or {}\n",
        "            cur = fsd.get(\"currentSpeed\")\n",
        "            if isinstance(cur, (int, float)) and cur > 0:\n",
        "                return float(cur)\n",
        "            ffs = fsd.get(\"freeFlowSpeed\")\n",
        "            if isinstance(ffs, (int, float)) and ffs > 0:\n",
        "                return float(ffs)\n",
        "            return 30.0\n",
        "        except requests.RequestException:\n",
        "            if attempt < retries:\n",
        "                attempt += 1\n",
        "                time.sleep(pause_sec * (attempt + 1))\n",
        "                continue\n",
        "            return 30.0\n",
        "\n",
        "\n",
        "def batch_get_tomtom_speeds(coords, pause_sec=0.1):\n",
        "    \"\"\"\n",
        "    coords: list of (lat, lon)\n",
        "    Returns: dict mapping (lat, lon) -> speed_kmh\n",
        "    \"\"\"\n",
        "    rounded = [(round(float(lat), 5), round(float(lon), 5)) for (lat, lon) in coords]\n",
        "    unique = list(dict.fromkeys(rounded))\n",
        "\n",
        "    speed_map = {}\n",
        "    for (rlat, rlon) in unique:\n",
        "        # if no API key available, simulate\n",
        "        if not TOMTOM_API_KEY:\n",
        "            speed_map[(rlat, rlon)] = max(5.0, float(np.random.normal(30, 6)))\n",
        "        else:\n",
        "            speed_map[(rlat, rlon)] = get_tomtom_speed_kmh(rlat, rlon)\n",
        "        time.sleep(pause_sec)\n",
        "\n",
        "    result = {}\n",
        "    for (lat, lon), (rlat, rlon) in zip(coords, rounded):\n",
        "        result[(float(lat), float(lon))] = speed_map[(rlat, rlon)]\n",
        "    return result\n",
        "\n",
        "\n",
        "# OPENWEATHER config (reads from env)\n",
        "OPENWEATHER_API_KEY = os.environ.get(\"OPENWEATHER_API_KEY\", \"596eadfb2fd863480d863b7062bd2e8f\")\n",
        "if not OPENWEATHER_API_KEY or OPENWEATHER_API_KEY.startswith(\"<\") or len(OPENWEATHER_API_KEY.strip()) == 0:\n",
        "    OPENWEATHER_API_KEY = OPENWEATHER_API_KEY  # keep the provided default\n",
        "\n",
        "OPENWEATHER_URL = \"https://api.openweathermap.org/data/2.5/weather\"\n",
        "\n",
        "\n",
        "def get_openweather(lat, lon, retries=2, pause_sec=0.5):\n",
        "    \"\"\"\n",
        "    Query OpenWeather current weather for (lat, lon).\n",
        "    Returns: dict with description, temp_c, humidity, wind_speed_ms, precip_mm_hr\n",
        "    If API key missing, returns simulated values.\n",
        "    \"\"\"\n",
        "    if not OPENWEATHER_API_KEY:\n",
        "        # simulate\n",
        "        desc = \"clear sky\"\n",
        "        return {\n",
        "            \"description\": desc,\n",
        "            \"temp_c\": float(np.random.normal(25, 4)),\n",
        "            \"humidity\": int(np.clip(np.random.normal(60, 10), 10, 100)),\n",
        "            \"wind_speed_ms\": float(np.abs(np.random.normal(2, 1))),\n",
        "            \"precip_mm_hr\": float(max(0.0, np.random.exponential(0.05))),\n",
        "        }\n",
        "\n",
        "    params = {\n",
        "        \"lat\": float(lat),\n",
        "        \"lon\": float(lon),\n",
        "        \"appid\": OPENWEATHER_API_KEY,\n",
        "        \"units\": \"metric\",\n",
        "    }\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            resp = requests.get(OPENWEATHER_URL, params=params, timeout=15)\n",
        "            if resp.status_code in (429, 500, 502, 503, 504):\n",
        "                if attempt < retries:\n",
        "                    attempt += 1\n",
        "                    time.sleep(pause_sec * (attempt + 1))\n",
        "                    continue\n",
        "                resp.raise_for_status()\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json() or {}\n",
        "\n",
        "            weather_list = data.get(\"weather\") or []\n",
        "            description = (weather_list[0].get(\"description\") if weather_list else \"\") or \"\"\n",
        "            main = data.get(\"main\") or {}\n",
        "            wind = data.get(\"wind\") or {}\n",
        "            rain = data.get(\"rain\") or {}\n",
        "            snow = data.get(\"snow\") or {}\n",
        "\n",
        "            temp_c = float(main.get(\"temp\")) if isinstance(main.get(\"temp\"), (int, float)) else np.nan\n",
        "            humidity = int(main.get(\"humidity\")) if isinstance(main.get(\"humidity\"), (int, float)) else np.nan\n",
        "            wind_ms = float(wind.get(\"speed\")) if isinstance(wind.get(\"speed\"), (int, float)) else np.nan\n",
        "\n",
        "            precip_1h = np.nan\n",
        "            if isinstance(rain.get(\"1h\"), (int, float)):\n",
        "                precip_1h = float(rain[\"1h\"])\n",
        "            elif isinstance(snow.get(\"1h\"), (int, float)):\n",
        "                precip_1h = float(snow[\"1h\"])\n",
        "\n",
        "            return {\n",
        "                \"description\": str(description).lower(),\n",
        "                \"temp_c\": temp_c,\n",
        "                \"humidity\": humidity,\n",
        "                \"wind_speed_ms\": wind_ms,\n",
        "                \"precip_mm_hr\": precip_1h if not np.isnan(precip_1h) else 0.0,\n",
        "            }\n",
        "        except requests.RequestException:\n",
        "            if attempt < retries:\n",
        "                attempt += 1\n",
        "                time.sleep(pause_sec * (attempt + 1))\n",
        "                continue\n",
        "            return {\n",
        "                \"description\": \"\",\n",
        "                \"temp_c\": np.nan,\n",
        "                \"humidity\": np.nan,\n",
        "                \"wind_speed_ms\": np.nan,\n",
        "                \"precip_mm_hr\": 0.0,\n",
        "            }\n",
        "\n",
        "\n",
        "def batch_get_openweather(coords, pause_sec=0.1):\n",
        "    rounded = [(round(float(lat), 5), round(float(lon), 5)) for (lat, lon) in coords]\n",
        "    unique = list(dict.fromkeys(rounded))\n",
        "\n",
        "    weather_map = {}\n",
        "    for (rlat, rlon) in unique:\n",
        "        weather_map[(rlat, rlon)] = get_openweather(rlat, rlon)\n",
        "        time.sleep(pause_sec)\n",
        "\n",
        "    result = {}\n",
        "    for (lat, lon), (rlat, rlon) in zip(coords, rounded):\n",
        "        result[(float(lat), float(lon))] = weather_map[(rlat, rlon)]\n",
        "    return result\n",
        "\n",
        "\n",
        "def compute_weather_factor(desc: str, precip_mm_hr: float) -> float:\n",
        "    d = (desc or \"\").lower()\n",
        "    if precip_mm_hr and precip_mm_hr > 0:\n",
        "        return 1.25\n",
        "    if any(k in d for k in [\"thunderstorm\", \"snow\", \"sleet\"]):\n",
        "        return 1.3\n",
        "    if \"rain\" in d or \"drizzle\" in d:\n",
        "        return 1.2\n",
        "    if \"mist\" in d or \"fog\" in d or \"haze\" in d or \"smoke\" in d:\n",
        "        return 1.1\n",
        "    return 1.0\n",
        "\n",
        "\n",
        "# ----------------- Utility functions -----------------\n",
        "\n",
        "def haversine_km(lat1, lon1, lat2, lon2):\n",
        "    # digit-by-digit haversine\n",
        "    R = 6371.0\n",
        "    lat1r, lon1r, lat2r, lon2r = map(radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2r - lat1r\n",
        "    dlon = lon2r - lon1r\n",
        "    a = sin(dlat / 2) ** 2 + cos(lat1r) * cos(lat2r) * sin(dlon / 2) ** 2\n",
        "    c = 2 * asin(sqrt(a))\n",
        "    return R * c\n",
        "\n",
        "\n",
        "# ----------------- Streamlit app -----------------\n",
        "\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Fleet assignments dashboard\")\n",
        "\n",
        "st.sidebar.title(\"Data & live options\")\n",
        "# UPDATED: default paths to the uploaded files\n",
        "assignments_csv = st.sidebar.text_input(\n",
        "    \"Assignments CSV path\",\n",
        "    \"/content/drive/MyDrive/assignments_log.csv\"\n",
        ")\n",
        "fleet_csv = st.sidebar.text_input(\n",
        "    \"Fleet CSV path\",\n",
        "    \"/content/drive/MyDrive/synthetic_fleet_with_coords (1).csv\"\n",
        ")\n",
        "demand_csv = st.sidebar.text_input(\n",
        "    \"Demand CSV path\",\n",
        "    \"/content/drive/MyDrive/synthetic_demand_data.csv\"\n",
        ")\n",
        "\n",
        "poll_cycles = st.sidebar.number_input(\"TomTom/OpenWeather poll cycles (per refresh)\", min_value=1, max_value=10, value=1, step=1)\n",
        "poll_interval_sec = st.sidebar.number_input(\"Poll interval seconds (between cycles)\", min_value=1, max_value=300, value=1, step=1)\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.write(\"API keys read from environment variables:\")\n",
        "st.sidebar.write(f\"TomTom key loaded: {'YES' if TOMTOM_API_KEY else 'NO'}\")\n",
        "st.sidebar.write(f\"OpenWeather key loaded: {'YES' if OPENWEATHER_API_KEY else 'NO'}\")\n",
        "\n",
        "if st.sidebar.button(\"Refresh live traffic & weather now\"):\n",
        "    st.experimental_rerun()\n",
        "\n",
        "@st.cache_data(ttl=30)\n",
        "def load_csvs(a_path, f_path, d_path):\n",
        "    a = pd.read_csv(a_path) if os.path.exists(a_path) else pd.DataFrame()\n",
        "    f = pd.read_csv(f_path) if os.path.exists(f_path) else pd.DataFrame()\n",
        "    d = pd.read_csv(d_path) if os.path.exists(d_path) else pd.DataFrame()\n",
        "    return a, f, d\n",
        "\n",
        "\n",
        "assn_df, fleet_df, demand_df = load_csvs(assignments_csv, fleet_csv, demand_csv)\n",
        "\n",
        "if assn_df.empty:\n",
        "    st.warning(\"Assignments CSV not found or empty. Provide assignments.csv path in sidebar.\")\n",
        "    st.stop()\n",
        "\n",
        "# prepare joined map DataFrame\n",
        "df_map = assn_df.copy()\n",
        "\n",
        "# attempt to merge lat/lon for demands and fleet if those are in separate files\n",
        "# We assume demand_df has columns: d_idx (or index), lat, lon\n",
        "# and fleet_df has columns: v_idx (or index), lat, lon\n",
        "if ('lat' not in df_map.columns or 'lon' not in df_map.columns) and (not demand_df.empty):\n",
        "    # try different possible column naming\n",
        "    if 'd_idx' in df_map.columns:\n",
        "        if 'd_idx' in demand_df.columns and {'lat', 'lon'}.issubset(demand_df.columns):\n",
        "            df_map = df_map.merge(demand_df[['d_idx', 'lat', 'lon']], on='d_idx', how='left')\n",
        "            df_map = df_map.rename(columns={'lat': 'd_lat', 'lon': 'd_lon'})\n",
        "        elif {'lat', 'lon'}.issubset(demand_df.columns):\n",
        "            # assume index alignment\n",
        "            demand_df = demand_df.reset_index().rename(columns={'index': 'd_idx'})\n",
        "            df_map = df_map.merge(demand_df[['d_idx', 'lat', 'lon']], on='d_idx', how='left')\n",
        "            df_map = df_map.rename(columns={'lat': 'd_lat', 'lon': 'd_lon'})\n",
        "\n",
        "if fleet_df is not None and not fleet_df.empty and 'v_idx' in df_map.columns:\n",
        "    if 'v_idx' in fleet_df.columns and {'lat', 'lon'}.issubset(fleet_df.columns):\n",
        "        fleet_coords = fleet_df[['v_idx', 'lat', 'lon']].rename(columns={'lat': 'v_lat', 'lon': 'v_lon'})\n",
        "        df_map = df_map.merge(fleet_coords, on='v_idx', how='left')\n",
        "    else:\n",
        "        # try index alignment\n",
        "        ff = fleet_df.reset_index().rename(columns={'index': 'v_idx'})\n",
        "        if {'v_idx', 'lat', 'lon'}.issubset(ff.columns):\n",
        "            ff = ff[['v_idx', 'lat', 'lon']].rename(columns={'lat': 'v_lat', 'lon': 'v_lon'})\n",
        "            df_map = df_map.merge(ff, on='v_idx', how='left')\n",
        "\n",
        "# fallback: if assn_df already had lat/lon for demand as 'lat'/'lon', copy to d_lat/d_lon\n",
        "if 'lat' in assn_df.columns and 'd_lat' not in df_map.columns:\n",
        "    df_map['d_lat'] = df_map['lat']\n",
        "    df_map['d_lon'] = df_map['lon']\n",
        "\n",
        "# center for maps\n",
        "if 'd_lat' in df_map.columns and 'd_lon' in df_map.columns:\n",
        "    center_lat = float(df_map['d_lat'].dropna().mean())\n",
        "    center_lon = float(df_map['d_lon'].dropna().mean())\n",
        "elif 'v_lat' in df_map.columns and 'v_lon' in df_map.columns:\n",
        "    center_lat = float(df_map['v_lat'].dropna().mean())\n",
        "    center_lon = float(df_map['v_lon'].dropna().mean())\n",
        "else:\n",
        "    center_lat, center_lon = 0.0, 0.0\n",
        "\n",
        "# Top metrics\n",
        "st.title(\"Fleet assignments dashboard\")\n",
        "c1, c2, c3, c4 = st.columns(4)\n",
        "c1.metric(\"Total reward\", round(assn_df['reward'].sum() if 'reward' in assn_df.columns else 0.0, 3))\n",
        "c2.metric(\"Avg ETA (min)\", round(assn_df['eta_min'].mean() if 'eta_min' in assn_df.columns else np.nan, 3))\n",
        "c3.metric(\"Avg distance (km)\", round(assn_df['dist_km'].mean() if 'dist_km' in assn_df.columns else np.nan, 3))\n",
        "c4.metric(\"Assignments\", len(assn_df))\n",
        "\n",
        "tabs = st.tabs([\"Map: assignments\", \"Capacity vs Demand\", \"Distance matrix\", \"Traffic (live)\", \"Weather (live)\"])\n",
        "\n",
        "# ---- Tab 1: assignments map ----\n",
        "with tabs[0]:\n",
        "    st.header(\"Assignments map (vehicle -> demand)\")\n",
        "    fol_map = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
        "    for _, r in df_map.iterrows():\n",
        "        lat_d = r.get('d_lat') or r.get('lat')\n",
        "        lon_d = r.get('d_lon') or r.get('lon')\n",
        "        lat_v = r.get('v_lat')\n",
        "        lon_v = r.get('v_lon')\n",
        "        popup_d = f\"d_idx: {r.get('d_idx', '')}<br>size: {r.get('dsize', '')}\"\n",
        "        popup_v = f\"v_idx: {r.get('v_idx', '')}<br>capacity: {r.get('capacity', '')}\"\n",
        "        try:\n",
        "            if pd.notnull(lat_d) and pd.notnull(lon_d):\n",
        "                folium.CircleMarker([lat_d, lon_d], radius=4, color='blue', fill=True, popup=popup_d).add_to(fol_map)\n",
        "            if pd.notnull(lat_v) and pd.notnull(lon_v):\n",
        "                folium.CircleMarker([lat_v, lon_v], radius=4, color='green', fill=True, popup=popup_v).add_to(fol_map)\n",
        "            if pd.notnull(lat_d) and pd.notnull(lon_d) and pd.notnull(lat_v) and pd.notnull(lon_v):\n",
        "                folium.PolyLine([[lat_v, lon_v], [lat_d, lon_d]], color='gray', weight=1).add_to(fol_map)\n",
        "        except Exception:\n",
        "            continue\n",
        "    st_folium(fol_map, width=1000)\n",
        "\n",
        "# ---- Tab 2: capacity vs demand ----\n",
        "with tabs[1]:\n",
        "    st.header(\"How demand size is satisfied by vehicle capacity\")\n",
        "    if {'dsize', 'capacity'}.issubset(assn_df.columns):\n",
        "        assn = assn_df.copy()\n",
        "        assn['utilization'] = assn['dsize'] / assn['capacity']\n",
        "        fig_hist = px.histogram(assn, x='utilization', nbins=30, title=\"Distribution of utilization (dsize / capacity)\")\n",
        "        st.plotly_chart(fig_hist, use_container_width=True)\n",
        "\n",
        "        agg = assn.groupby('v_idx', dropna=False).agg(total_demand=pd.NamedAgg('dsize', 'sum'),\n",
        "                                                       capacity=pd.NamedAgg('capacity', 'first')).reset_index()\n",
        "        agg['remaining'] = agg['capacity'] - agg['total_demand']\n",
        "        fig_bar = px.bar(agg, x='v_idx', y=['total_demand', 'remaining'], title=\"Per vehicle: served vs remaining capacity\")\n",
        "        st.plotly_chart(fig_bar, use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"Columns 'dsize' and 'capacity' required in assignments.csv to show capacity utilization.\")\n",
        "\n",
        "# ---- Tab 3: distance matrix / per-assignment distances ----\n",
        "with tabs[2]:\n",
        "    st.header(\"Distance between fleet and demand (per assignment) and pairwise fleet distances\")\n",
        "    dfd = df_map.copy()\n",
        "    # ensure dlat/dlon and vlat/vlon exist\n",
        "    dfd['dlat'] = dfd.get('d_lat', dfd.get('lat'))\n",
        "    dfd['dlon'] = dfd.get('d_lon', dfd.get('lon'))\n",
        "    dfd['vlat'] = dfd.get('v_lat')\n",
        "    dfd['vlon'] = dfd.get('v_lon')\n",
        "    if dfd[['dlat', 'dlon']].notna().any().any() and dfd[['vlat', 'vlon']].notna().any().any():\n",
        "        dfd['dist_calc_km'] = dfd.apply(lambda r: haversine_km(r['vlat'], r['vlon'], r['dlat'], r['dlon']) if pd.notnull(r['vlat']) and pd.notnull(r['dlat']) else np.nan, axis=1)\n",
        "        st.dataframe(dfd[['d_idx', 'v_idx', 'dist_km', 'dist_calc_km']].head(300))\n",
        "        # pairwise fleet centroid distances\n",
        "        try:\n",
        "            fleet_centroids = dfd.groupby('v_idx').agg(vlat=('vlat','mean'), vlon=('vlon','mean')).dropna()\n",
        "            idxs = list(fleet_centroids.index)\n",
        "            mat = np.zeros((len(idxs), len(idxs)))\n",
        "            for i, vi in enumerate(idxs):\n",
        "                for j, vj in enumerate(idxs):\n",
        "                    mat[i,j] = haversine_km(fleet_centroids.loc[vi,'vlat'], fleet_centroids.loc[vi,'vlon'],\n",
        "                                            fleet_centroids.loc[vj,'vlat'], fleet_centroids.loc[vj,'vlon'])\n",
        "            figm = px.imshow(mat, x=idxs, y=idxs, labels=dict(x=\"v_idx\", y=\"v_idx\", color=\"km\"), title=\"Pairwise fleet centroid distances (km)\")\n",
        "            st.plotly_chart(figm, use_container_width=True)\n",
        "        except Exception as e:\n",
        "            st.write(\"Could not compute pairwise fleet matrix:\", e)\n",
        "    else:\n",
        "        st.info(\"Need vehicle and demand lat/lon (v_lat/v_lon and d_lat/d_lon) to compute distances.\")\n",
        "\n",
        "# ---- helper: fetch live traffic & weather DataFrames ----\n",
        "\n",
        "def fetch_live_traffic_from_tomtom(demand_points, poll_cycles_local=1, interval_sec_local=1):\n",
        "    \"\"\"\n",
        "    demand_points: pandas DataFrame with columns 'dlat','d_lon' or 'lat','lon' (demand points).\n",
        "    Returns DataFrame with columns lat, lon, traffic_speed_kmh\n",
        "    \"\"\"\n",
        "    if demand_points is None or demand_points.empty:\n",
        "        return pd.DataFrame(columns=['lat','lon','traffic_speed_kmh'])\n",
        "\n",
        "    coords = []\n",
        "    if {'d_lat','d_lon'}.issubset(demand_points.columns):\n",
        "        coords = [(float(r.d_lat), float(r.d_lon)) for _, r in demand_points.iterrows()]\n",
        "    elif {'lat','lon'}.issubset(demand_points.columns):\n",
        "        coords = [(float(r.lat), float(r.lon)) for _, r in demand_points.iterrows()]\n",
        "    else:\n",
        "        return pd.DataFrame(columns=['lat','lon','traffic_speed_kmh'])\n",
        "\n",
        "    last_df = None\n",
        "    for i in range(1, poll_cycles_local + 1):\n",
        "        tt_speed_by_coord = batch_get_tomtom_speeds(coords, pause_sec=0.1)\n",
        "        rows = []\n",
        "        for (lat, lon) in coords:\n",
        "            speed = tt_speed_by_coord.get((float(lat), float(lon)), None)\n",
        "            rows.append({\"lat\": float(lat), \"lon\": float(lon), \"traffic_speed_kmh\": float(speed)})\n",
        "        last_df = pd.DataFrame(rows)\n",
        "        # small print/log\n",
        "        st.info(f\"[{datetime.datetime.now().isoformat()}] TomTom speeds fetched (cycle {i}/{poll_cycles_local}).\")\n",
        "        if i < poll_cycles_local:\n",
        "            time.sleep(interval_sec_local)\n",
        "    return last_df\n",
        "\n",
        "def fetch_live_weather_from_openweather(demand_points, poll_cycles_local=1, interval_sec_local=1):\n",
        "    \"\"\"\n",
        "    demand_points: pandas DataFrame with columns 'd_lat','d_lon' or 'lat','lon'\n",
        "    Returns DataFrame with lat, lon, description, temp_c, humidity, wind_speed_ms, precip_mm_hr, weather_factor\n",
        "    \"\"\"\n",
        "    if demand_points is None or demand_points.empty:\n",
        "        return pd.DataFrame(columns=['lat','lon','description','temp_c','humidity','wind_speed_ms','precip_mm_hr','weather_factor'])\n",
        "\n",
        "    coords = []\n",
        "    if {'d_lat','d_lon'}.issubset(demand_points.columns):\n",
        "        coords = [(float(r.d_lat), float(r.d_lon)) for _, r in demand_points.iterrows()]\n",
        "    elif {'lat','lon'}.issubset(demand_points.columns):\n",
        "        coords = [(float(r.lat), float(r.lon)) for _, r in demand_points.iterrows()]\n",
        "    else:\n",
        "        return pd.DataFrame(columns=['lat','lon','description','temp_c','humidity','wind_speed_ms','precip_mm_hr','weather_factor'])\n",
        "\n",
        "    last_df = None\n",
        "    for i in range(1, poll_cycles_local + 1):\n",
        "        ow_by_coord = batch_get_openweather(coords, pause_sec=0.1)\n",
        "        rows = []\n",
        "        for (lat, lon) in coords:\n",
        "            w = ow_by_coord.get((float(lat), float(lon)), {})\n",
        "            desc = w.get('description', '')\n",
        "            temp_c = w.get('temp_c', np.nan)\n",
        "            humidity = w.get('humidity', np.nan)\n",
        "            wind_ms = w.get('wind_speed_ms', np.nan)\n",
        "            precip = w.get('precip_mm_hr', 0.0)\n",
        "            factor = compute_weather_factor(desc, precip)\n",
        "            rows.append({\"lat\": float(lat), \"lon\": float(lon),\n",
        "                         \"description\": desc, \"temp_c\": temp_c, \"humidity\": humidity,\n",
        "                         \"wind_speed_ms\": wind_ms, \"precip_mm_hr\": precip, \"weather_factor\": factor})\n",
        "        last_df = pd.DataFrame(rows)\n",
        "        st.info(f\"[{datetime.datetime.now().isoformat()}] OpenWeather fetched (cycle {i}/{poll_cycles_local}).\")\n",
        "        if i < poll_cycles_local:\n",
        "            time.sleep(interval_sec_local)\n",
        "    return last_df\n",
        "\n",
        "# ---- Tab 4: traffic (live) ----\n",
        "with tabs[3]:\n",
        "    st.header(\"Traffic speed plotted on demand locations (TomTom)\")\n",
        "    # choose demand sample points (use df_map)\n",
        "    demand_points = df_map\n",
        "    traffic_df = fetch_live_traffic_from_tomtom(demand_points, poll_cycles_local=poll_cycles, interval_sec_local=poll_interval_sec)\n",
        "    if traffic_df is not None and not traffic_df.empty:\n",
        "        # pydeck scatter\n",
        "        try:\n",
        "            view = pdk.ViewState(latitude=float(traffic_df['lat'].mean()), longitude=float(traffic_df['lon'].mean()), zoom=12)\n",
        "            layer = pdk.Layer(\"ScatterplotLayer\", data=traffic_df, get_position='[lon, lat]', get_radius=80,\n",
        "                              get_fill_color='[255 - (traffic_speed_kmh*5), (traffic_speed_kmh*5), 60, 180]', pickable=True)\n",
        "            deck = pdk.Deck(layers=[layer], initial_view_state=view, tooltip={\"text\":\"Speed: {traffic_speed_kmh} km/h\"})\n",
        "            st.pydeck_chart(deck, use_container_width=True)\n",
        "        except Exception:\n",
        "            # fallback to folium\n",
        "            fmap = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
        "            for _, r in traffic_df.iterrows():\n",
        "                folium.CircleMarker([r.lat, r.lon], radius=6, popup=f\"speed: {r.traffic_speed_kmh} km/h\").add_to(fmap)\n",
        "            st_folium(fmap, width=900)\n",
        "        st.dataframe(traffic_df.head())\n",
        "    else:\n",
        "        st.info(\"No traffic results (TomTom key missing or no demand points).\")\n",
        "\n",
        "# ---- Tab 5: weather (live) ----\n",
        "with tabs[4]:\n",
        "    st.header(\"Weather plotted on demand locations (OpenWeather)\")\n",
        "    weather_df = fetch_live_weather_from_openweather(df_map, poll_cycles_local=poll_cycles, interval_sec_local=poll_interval_sec)\n",
        "    if weather_df is not None and not weather_df.empty:\n",
        "        fmap = folium.Map(location=[float(weather_df['lat'].mean()), float(weather_df['lon'].mean())], zoom_start=12)\n",
        "        for _, r in weather_df.iterrows():\n",
        "            popup = f\"desc: {r.get('description')}<br>temp: {r.get('temp_c')}°C<br>precip: {r.get('precip_mm_hr')} mm/h\"\n",
        "            folium.CircleMarker([r.lat, r.lon], radius=6, popup=popup, fill=True).add_to(fmap)\n",
        "        st_folium(fmap, width=1000)\n",
        "        st.dataframe(weather_df.head())\n",
        "    else:\n",
        "        st.info(\"No weather results (OpenWeather key missing or no demand points).\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"Notes: live TomTom/OpenWeather calls are rate-limited in the helper functions. Use poll_cycles & interval in the sidebar to control how many times and spacing between polls. If you don't set API keys, the app will simulate traffic/weather values.\")\n",
        "# Colab cell (python)\n",
        "import time, os, signal, subprocess\n",
        "# ensure previous log is removed for clarity\n",
        "if os.path.exists(\"streamlit.log\"):\n",
        "    os.remove(\"streamlit.log\")\n",
        "\n",
        "# start streamlit in the background and redirect logs\n",
        "cmd = \"streamlit run streamlit_app.py --server.port 8501 --server.enableCORS false --server.headless true\"\n",
        "print(\"Launching Streamlit with command:\", cmd)\n",
        "\n",
        "# use nohup to keep process alive and write logs to streamlit.log\n",
        "get_ipython().system_raw(f\"nohup {cmd} > streamlit.log 2>&1 &\")\n",
        "\n",
        "# wait a moment for server to start\n",
        "time.sleep(3)\n",
        "print(\"Started Streamlit; waiting 3s then checking localhost:8501 ...\")\n",
        "\n",
        "# quick curl check (prints headers or 'connection refused')\n",
        "#!curl -s -I http://localhost:8501 | sed -n '1,12p' || echo \"curl failed to connect\"\n",
        "print(\"\\nIf you see HTTP headers above (like HTTP/1.1 200 or 302), Streamlit is listening at localhost:8501.\")\n",
        "print(\"If curl failed or connection refused, open the streamlit.log file below to see errors.\")\n",
        "# show last lines of the log\n",
        "print(\"\\n--- streamlit.log (tail) ---\")\n",
        "#!tail -n 60 streamlit.log || true\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "NGROK_AUTH_TOKEN = \"32FvHNzjMh8tcnakAfPbw8PBn3D_6dJdLXgx91REcwmJPRFoM\"   # <-- paste your token here (keep secret)\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# ensure no previous tunnels\n",
        "ngrok.kill()\n",
        "time.sleep(0.5)\n",
        "\n",
        "# 4) Launch streamlit in background and create public URL\n",
        "get_ipython().system_raw('streamlit run streamlit_app.py --server.port 8501 &')\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(\"Streamlit is running at:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr8DLc5uFaxN",
        "outputId": "f700b9d8-5b8b-4c22-e63f-6bd4547ba424"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit is running at: NgrokTunnel: \"https://cd8e52fc002a.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}